{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "from deepeval import evaluate\n",
    "\n",
    "class OllamaLlama3_2(DeepEvalBaseLLM):\n",
    "    def __init__(self, model_name='llama3.2'):\n",
    "        self.model_name = model_name\n",
    "        self.api_url = 'http://localhost:11434/api/generate'  # Ollama's API endpoint\n",
    "\n",
    "    def load_model(self):\n",
    "        # No need to load the model explicitly; Ollama handles this\n",
    "        pass\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        response = requests.post(self.api_url, json={\n",
    "            'model': self.model_name,\n",
    "            'prompt': prompt,\n",
    "            'stream' : False\n",
    "        })\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"Raw response:\", response.text)  # Print raw response for inspection\n",
    "            try:\n",
    "                return response.json().get('text', '').strip()\n",
    "            except ValueError as e:\n",
    "                print(f\"Error parsing JSON: {e}\")\n",
    "                return ''\n",
    "        else:\n",
    "            raise Exception(f\"Error from Ollama API: {response.status_code} - {response.text}\")\n",
    "\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        # Implement asynchronous generation if needed\n",
    "        return self.generate(prompt)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return self.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response: {\"model\":\"llama3.2\",\"created_at\":\"2025-03-30T09:53:12.58176Z\",\"response\":\"I'm just a language model, so I don't have feelings or emotions like humans do, but thank you for asking! How can I assist you today? Is there something on your mind that you'd like to talk about, or perhaps some information you're looking for? I'm here to help!\",\"done\":true,\"done_reason\":\"stop\",\"context\":[128006,9125,128007,271,38766,1303,33025,2696,25,6790,220,2366,18,271,128009,128006,882,128007,271,9906,11,1268,527,499,30,128009,128006,78191,128007,271,40,2846,1120,264,4221,1646,11,779,358,1541,956,617,16024,477,21958,1093,12966,656,11,719,9901,499,369,10371,0,2650,649,358,7945,499,3432,30,2209,1070,2555,389,701,4059,430,499,4265,1093,311,3137,922,11,477,8530,1063,2038,499,2351,3411,369,30,358,2846,1618,311,1520,0],\"total_duration\":3251118250,\"load_duration\":36531542,\"prompt_eval_count\":31,\"prompt_eval_duration\":514060542,\"eval_count\":62,\"eval_duration\":2696725625}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "\n",
    "# Initialize the evaluator with your custom LLM\n",
    "ollama = OllamaLlama3_2()\n",
    "evaluator = AnswerRelevancyMetric(model=ollama)\n",
    "ollama.generate(\"Hello, how are you?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
